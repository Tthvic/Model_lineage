# Adaptive Attacks - Overview

This directory contains implementations of various **adaptive attacks** designed to test the robustness of our model lineage attestation framework.

## Attack Categories

We implement adaptive attacks for two model types:

### 1. Large Language Models (LLMs)
- **Knowledge Overwriting Attack**: Attacker uses a different model to generate answers and finetunes the target model
- **Knowledge Infusion Attack**: Injects varying amounts of knowledge from one model into another to test lineage detection

### 2. Small Vision Models
- **Knowledge Overwriting Attack**: Training with shuffled labels to overwrite learned representations
- **Weight Perturbation Attack**: Adding noise to model parameters to obfuscate lineage

## Directory Structure

```
adaptive_attacks/
├── README.md                          # This file
├── llm/                               # LLM adaptive attacks
│   ├── knowledge_overwriting/         # Knowledge overwriting for LLMs
│   │   ├── README.md
│   │   ├── config.py
│   │   ├── step1_generate_qa_pairs.py
│   │   ├── step2_finetune_models.py
│   │   ├── step3_generate_answers.py
│   │   ├── step4_generate_embeddings.py
│   │   ├── step5_compute_differences.py
│   │   ├── step6_compute_similarity.py
│   │   └── run_experiment.sh
│   └── knowledge_infusion/            # Knowledge infusion for LLMs
│       ├── README.md
│       ├── config.py
│       ├── step1_split_data.py
│       ├── step2_finetune_models.py
│       ├── step3_generate_answers.py
│       ├── step4_generate_embeddings.py
│       ├── step5_compute_ba_diff.py
│       ├── step6_compute_similarity.py
│       └── run_experiment.sh
└── small_model/                       # Small model adaptive attacks
    ├── knowledge_overwriting/         # Knowledge overwriting for vision models
    │   ├── README.md
    │   ├── config.py
    │   ├── attack_mobilenet.py
    │   └── run_experiment.sh
    └── weight_perturbation/           # Weight perturbation attack
        ├── README.md
        ├── config.py
        ├── attack_caltech.py
        └── run_experiment.sh
```

## Quick Start

### Prerequisites

1. **Environment Setup**
   ```bash
   conda create -n lineage python=3.12 -y
   conda activate lineage
   pip install -r requirements.txt
   ```

2. **Download Models and Datasets**
   - For LLM attacks: See `llm/knowledge_overwriting/README.md`
   - For small model attacks: See `small_model/knowledge_overwriting/README.md`

### Running Attacks

#### LLM Knowledge Overwriting Attack
```bash
cd adaptive_attacks/llm/knowledge_overwriting
bash run_experiment.sh
```

#### LLM Knowledge Infusion Attack
```bash
cd adaptive_attacks/llm/knowledge_infusion
bash run_experiment.sh
```

#### Small Model Knowledge Overwriting Attack
```bash
cd adaptive_attacks/small_model/knowledge_overwriting
bash run_experiment.sh
```

#### Small Model Weight Perturbation Attack
```bash
cd adaptive_attacks/small_model/weight_perturbation
bash run_experiment.sh
```

## Attack Descriptions

### Knowledge Overwriting Attack (LLM)

**Scenario**: Attacker knows the test set and uses a different model (Llama-3.1-8B-Instruct) to generate answers. Then finetunes the target model (Qwen2.5-1.5B-Policy2) with these answers to inject knowledge.

**Three Intensities**:
- Low (10%): Finetune with 20 QA pairs
- Medium (30%): Finetune with 60 QA pairs
- High (50%): Finetune with 100 QA pairs

**Expected Result**: Lineage similarity > 0.4 even under attack

### Knowledge Infusion Attack (LLM)

**Scenario**: Tests how knowledge inheritance affects lineage similarity by finetuning one model (Model B) with varying amounts of QA data generated by another model (Model A). Creates variants B1, B2, B3 with increasing knowledge infusion.

**Three Infusion Levels**:
- B1 (Low): 5 QA pairs from Model A
- B2 (Medium): 10 QA pairs from Model A  
- B3 (High): 20 QA pairs from Model A

**Hypothesis**: More knowledge from A → Higher lineage similarity with A

**Method**: 
1. Split QA data from Model A into different subsets
2. Finetune Model B with each subset using LoRA
3. Generate embeddings for all models on test questions
4. Compute lineage similarity using relation network

**Expected Result**: Lineage similarity increases monotonically: Sim(A,B1) < Sim(A,B2) < Sim(A,B3)

### Knowledge Overwriting Attack (Small Model)

**Scenario**: Attacker finetunes the child model with shuffled/randomized labels to overwrite the learned knowledge from parent model.

**Method**: 
- Load pre-trained child model
- Train with randomly shuffled labels
- Evaluate lineage detection robustness

### Weight Perturbation Attack (Small Model)

**Scenario**: Attacker adds carefully crafted noise to model weights to obfuscate the lineage while maintaining model performance.

**Method**:
- Load fine-tuned model
- Add controlled Gaussian noise to weights
- Test lineage detection under perturbation

## Evaluation Metrics

All attacks are evaluated using:

1. **Lineage Similarity**: Cosine similarity between parent and predicted parent from child
2. **True Positive Rate (TPR)**: Correctly identified lineage relationships
3. **False Positive Rate (FPR)**: Incorrectly identified non-lineage as lineage
4. **Accuracy**: Overall classification accuracy

## Expected Robustness

Our lineage attestation framework is designed to be robust against these attacks:

| Attack Type | Model Type | Expected Detection Rate |
|-------------|------------|------------------------|
| Knowledge Overwriting | LLM | > 0.4 similarity |
| Knowledge Infusion | LLM | > 0.4 similarity |
| Knowledge Overwriting | Small Model | > 80% accuracy |
| Weight Perturbation | Small Model | > 75% accuracy |

## Citation

If you use these adaptive attack implementations, please cite:

```bibtex
@article{model_lineage_2025,
  title={Attesting Model Lineage by Consisted Knowledge Evolution with Fine-Tuning Trajectory},
  author={Zhuoyi Shang, Jiasen Li, Pengzhen Chen, Yanwei Liu, Xiaoyan Gu, Weiping Wang},
  year={2025}
}
```

## Contributing

When adding new adaptive attacks:

1. Create a new subdirectory under appropriate model type
2. Include `README.md`, `config.py`, and `run_experiment.sh`
3. Use relative paths for reproducibility
4. Document expected results clearly
5. Provide one-click execution scripts

## Support

For issues or questions about adaptive attacks:
- Check individual attack READMEs for specific details
- Ensure all required models and datasets are downloaded
- Verify configuration paths match your setup
