# Structure Evasion via Distillation

This folder implements the attack/defense described in the paper:

1. An attacker steals the parent model \( f_P \), fine-tunes it to obtain descendant models \( f_C^\mathcal{A} \) (already saved in `train_mobilenet/Pmodels/Calt_*.pth`).
2. To evade lineage checks, the attacker distills each \( f_C^\mathcal{A} \) into a small three-convolution student \( f_C^{\mathcal{A}'} \) using the script below.
3. For defense, we reverse-distill \( f_C^{\mathcal{A}'} \) back into the parent architecture (Mobilenet) to obtain a proxy \( f_C^{\mathcal{A}''} \) and measure its similarity to \( f_P \).

---

## Attacker Script: `distill_calt_sys_fan.py`

This script assumes the attacker already has the descendant checkpoints `Calt_*.pth`.

1. **Teacher setup.** For each descendant file, we load a compact teacher `DistillNet` (three convolutional layers by default). You can also reuse an already-trained compact teacher from `distilled_model3_synth/`.
2. **Student setup.** The student is a Mobilenet initialized with parent weights (`load_pmodels(..., initFlag=True)`), representing the architecture transformation target.
3. **Training** (`train_distill`): We perform distillation using `distillation_loss` (Cross-Entropy + KL divergence) on either real data (`get_data_loader`) or synthetic data generated by `synthesize_images` (deep inversion). Use the commented-out block if you want data-free training.
4. **Outputs.** Each distilled student is written to `./distilled_model3_synth_fans/Calt_<idx>.pth`.

Command example:
```bash
python distill_calt_sys_fan.py --device cuda:0
